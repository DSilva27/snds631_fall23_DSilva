{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szhvRicwXttV"
   },
   "source": [
    "### Generate $m$ data points $\\{y_i, x_i\\}$ satisfying $y_i = x_i^\\top \\beta$, where $\\beta \\in \\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "k6YS7B2ZXqUJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "seed = 431\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "def generate_data(m, n):\n",
    "    \"\"\"\n",
    "    Generate data for a noiseless linear model y = X beta.\n",
    "\n",
    "    Parameters:\n",
    "    - m: Number of data points (rows of X).\n",
    "    - n: Number of features (columns of X and size of beta).\n",
    "\n",
    "    Returns:\n",
    "    - X: Random matrix of size m x n.\n",
    "    - y: Vector of size m obtained using the linear model.\n",
    "    - beta: Gaussian random vector of size n.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate random matrix X of size m x n\n",
    "    X = np.random.rand(m, n)\n",
    "\n",
    "    # Generate Gaussian random vector beta of size n\n",
    "    beta = np.random.randn(n)\n",
    "\n",
    "    # Compute y using the linear model\n",
    "    y = X @ beta\n",
    "\n",
    "    return X, y, beta\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# m, n = 100, 5\n",
    "# X, y, beta = generate_data(m, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBnk6dq8YXyT"
   },
   "source": [
    "## Consider the rank deficient case where $m < n$. Show that there are multiple solutions to the linear equation $y = X \\beta$.\n",
    "\n",
    "We generate some data $X, y$ with $m = 50$ and $n = 200$. We can see that $y = X b$ has at least two solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PTNoLEeCYMe1",
    "outputId": "ead37437-1228-449c-8ec6-71380f526284"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1 and b2 are solutions to $y = X *beta$ because the residue are 5.520812141926581e-14,6.885540186746371e-14\n"
     ]
    }
   ],
   "source": [
    "def find_two_solutions_corrected(X, y):\n",
    "    # Compute the pseudo-inverse of X\n",
    "    X_pseudo = np.linalg.pinv(X)\n",
    "\n",
    "    # Compute the minimum norm solution\n",
    "    beta_min_norm = np.dot(X_pseudo, y)\n",
    "\n",
    "    # Compute a basis for the null space of X\n",
    "    _, _, Vt = np.linalg.svd(X)\n",
    "    null_space_basis = Vt[-(X.shape[1] - np.linalg.matrix_rank(X)) :].T\n",
    "\n",
    "    # Generate a vector in the null space of X\n",
    "    arbitrary_coefficients = np.random.rand(null_space_basis.shape[1])\n",
    "    null_space_vector = np.dot(null_space_basis, arbitrary_coefficients)\n",
    "\n",
    "    # Compute the arbitrary solution\n",
    "    beta_arbitrary = beta_min_norm + null_space_vector\n",
    "\n",
    "    return beta_min_norm, beta_arbitrary\n",
    "\n",
    "\n",
    "m = 50\n",
    "n = 200\n",
    "X, y, beta = generate_data(m, n)\n",
    "b1, b2 = find_two_solutions_corrected(X, y)\n",
    "\n",
    "print(\n",
    "    f\"b1 and b2 are solutions to $y = X *beta$ because the residue are {np.linalg.norm(y - X@b1)},{np.linalg.norm(y - X@b2)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtW-g1GEdhpT"
   },
   "source": [
    "# Min-norm solution via cvxpy\n",
    "\n",
    "The mathematical formulation for finding the minimum norm solution $ \\mathbf{b} $ subject to the constraint $ y = X \\mathbf{b} $ is as follows:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{minimize} \\quad & \\lVert \\mathbf{b} \\rVert_2 \\\\\n",
    "\\text{subject to} \\quad & X \\mathbf{b} = y\n",
    "\\end{align*}\n",
    "\n",
    "Where:\n",
    "- $ \\lVert \\mathbf{b} \\rVert_2 $ is the Euclidean norm of $ \\mathbf{b} $.\n",
    "- $ X $ is the given matrix.\n",
    "- $ y $ is the given vector.\n",
    "\n",
    "\n",
    "\n",
    "Once you run this code in your local environment with CVXPY installed, it will return the minimum norm solution $ \\mathbf{b} $ for the equation $ y = X \\mathbf{b} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "clcvoKP5aOvm"
   },
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "\n",
    "\n",
    "def min_norm_solution_cvxpy(X, y):\n",
    "    # Define the variable\n",
    "    b = cp.Variable(X.shape[1])\n",
    "\n",
    "    # Your codes start here\n",
    "    problem = cp.Problem(cp.Minimize(cp.norm(b)), [X @ b == y])\n",
    "    problem.solve()\n",
    "\n",
    "    return b.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oAaqI9q_eFsp"
   },
   "source": [
    "# Gradient descent starting from zero.\n",
    "\n",
    " The gradient descent algorithm updates the parameters iteratively using the gradient of the objective function with respect to the parameters.\n",
    "\n",
    "For the linear system $ y = X \\beta $ and the objective function $J(\\beta) = \\frac{1}{m} \\lVert X \\beta - y \\rVert_2^2 $, the gradient with respect to $ \\beta $ is:\n",
    "$$\n",
    "\\nabla J(\\beta) =\\frac{2}{m} X^T (X \\beta - y)\n",
    "$$\n",
    "\n",
    "Using the gradient descent update rule, we can iteratively update the solution $ \\beta $ as:\n",
    "\n",
    "$$\n",
    "\\beta_{\\text{new}} = \\beta_{\\text{old}} - \\alpha \\nabla J(\\beta)\n",
    "$$\n",
    "\n",
    "Where $ \\alpha $ is the learning rate. You are going to implement gradient descent, initialized from a zero vector.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "unhji_59d_bm"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, learning_rate=0.001, num_iterations=10000):\n",
    "    # Initialize beta as a zero vector\n",
    "    beta = np.zeros(X.shape[1])\n",
    "    # Your code starts here, return the final iterate of gradient descnet algorithm, intialize beta as a zero vector\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        grad = 2 / X.shape[0] * X.T @ (X @ beta - y)\n",
    "        beta = beta - learning_rate * grad\n",
    "\n",
    "    return beta\n",
    "\n",
    "\n",
    "def loss_and_gradient(X, y, beta):\n",
    "    # compute least-squres loss and the norm of gradient\n",
    "    # Your code starts here\n",
    "    # compute the loss function $f(\\beta) = \\frac{1}{2m} \\|y - X \\beta\\|_2^2$. Note that you need to normalize by $m$.\n",
    "    # compute the gradient of the loss function $\\nabla f(\\beta) = \\frac{1}{m} X^T (X \\beta - y)$\n",
    "    X_beta = X @ beta\n",
    "\n",
    "    loss = np.linalg.norm(X_beta - y) ** 2 / X.shape[0]\n",
    "    grad_norm = 2 / X.shape[0] * np.linalg.norm(X.T @ (X_beta - y))\n",
    "\n",
    "    return loss, grad_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWt4VJCLe0IN"
   },
   "source": [
    "Now test thse two functions. We want to show that gradient descent finds the min-norm solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vtsgdq8JevPv",
    "outputId": "f828ca2b-5169-4657-eb7c-fab92230a050"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the differene between the two solutions are bounded by 8.46207335792476e-09\n",
      "gradient descent with a very small learning rate coincides with the min-norm solution\n",
      "the loss and gradient of loss of GD solution is (6.2284596554881515e-18, 1.4775673773399028e-09)\n"
     ]
    }
   ],
   "source": [
    "m = 50\n",
    "n = 200\n",
    "X, y, beta = generate_data(m, n)\n",
    "\n",
    "beta_min_norm = min_norm_solution_cvxpy(X, y)\n",
    "beta_gd = gradient_descent(X, y, learning_rate=0.002, num_iterations=50000)\n",
    "error = np.linalg.norm(beta_gd - beta_min_norm)\n",
    "if error < 1e-5:\n",
    "    print(f\"the differene between the two solutions are bounded by {error}\")\n",
    "    print(\n",
    "        \"gradient descent with a very small learning rate coincides with the min-norm solution\"\n",
    "    )\n",
    "    print(\n",
    "        f\"the loss and gradient of loss of GD solution is {loss_and_gradient(X, y, beta_gd)}\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"the differene between the two solutions are bounded by {error}\")\n",
    "    print(\"gradient descent does not coincide with min-norm solution\")\n",
    "    print(\n",
    "        f\"the loss and gradient of loss of GD solution is {loss_and_gradient(X, y, beta_gd)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uQmxr0wDfYZC",
    "outputId": "a3e80d24-d00c-4b79-9297-ff422e736734"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the differene between the two solutions are bounded by 2.36483610505301e-10\n",
      "gradient descent with a very small learning rate coincides with the min-norm solution\n",
      "the loss and gradient of loss of GD solution is (3.490222630513036e-28, 1.4475067301903562e-14)\n"
     ]
    }
   ],
   "source": [
    "m = 50\n",
    "n = 200\n",
    "X, y, beta = generate_data(m, n)\n",
    "\n",
    "\n",
    "beta_min_norm = min_norm_solution_cvxpy(X, y)\n",
    "beta_gd2 = gradient_descent(X, y, learning_rate=0.01, num_iterations=50000)\n",
    "error = np.linalg.norm(beta_gd2 - beta_min_norm)\n",
    "\n",
    "if error < 1e-5:\n",
    "    print(f\"the differene between the two solutions are bounded by {error}\")\n",
    "    print(\n",
    "        \"gradient descent with a very small learning rate coincides with the min-norm solution\"\n",
    "    )\n",
    "    print(\n",
    "        f\"the loss and gradient of loss of GD solution is {loss_and_gradient(X, y, beta_gd2)}\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"the differene between the two solutions are bounded by {error}\")\n",
    "    print(\"gradient descent does not coincide with min-norm solution\")\n",
    "    print(\n",
    "        f\"the loss and gradient of loss of GD solution is {loss_and_gradient(X, y, beta_gd2)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_1IfqwAP6iL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
